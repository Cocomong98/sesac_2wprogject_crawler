import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# --- ì„¤ì • ---
NEWS_DIR = "news"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

def setup_directory():
    """ë‰´ìŠ¤ ì €ì¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
    if not os.path.exists(NEWS_DIR):
        os.makedirs(NEWS_DIR)

def get_article_detail(url):
    """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ê³¼ ë‚ ì§œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        response = requests.get(url, headers=HEADERS)
        response.encoding = 'utf-8' 
        soup = BeautifulSoup(response.text, 'html.parser')

        # 1. ë°°í¬ ì¼ì ì¶”ì¶œ
        date_tag = soup.select_one('.media_end_head_info_dateline_time, .date, .t11')
        date_str = date_tag.get_text(strip=True) if date_tag else "ë‚ ì§œ ì •ë³´ ì—†ìŒ"

        # 2. ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content_tag = soup.select_one('#dic_area, #articleBodyContents, #newsEndContents')
        if content_tag:
            # ê¸°ì‚¬ ë‚´ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for s in content_tag.select('script, style, span.end_photo_org'):
                s.decompose()
            content = content_tag.get_text("\n", strip=True)
        else:
            content = "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return date_str, content
    except:
        return "ë‚ ì§œ ì •ë³´ ì—†ìŒ", "ê¸°ì‚¬ ì½ê¸° ì‹¤íŒ¨"

def fetch_stock_news_by_code(code):
    """ì…ë ¥ë°›ì€ ì¢…ëª© ì½”ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ì½”ë“œë¥¼ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print(f"\nğŸš€ ì¢…ëª© ì½”ë“œ [{code}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ì‹¤ì œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ê°€ ìˆëŠ” iframe ì£¼ì†Œ
    list_url = f"https://finance.naver.com/item/news_news.naver?code={code}"
    
    try:
        response = requests.get(list_url, headers=HEADERS)
        response.encoding = 'euc-kr'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        links = soup.select('td.title a')
        
        if not links:
            print(f"âŒ í•´ë‹¹ ì¢…ëª©({code})ì˜ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return

        collected_news = []
        for link in links[:5]:  # ìµœì‹  5ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
            title = link.get_text(strip=True)
            href = link['href']
            
            # ì ˆëŒ€ ê²½ë¡œ ë³´ì •
            if href.startswith('/'):
                article_url = "https://finance.naver.com" + href
            else:
                article_url = href

            # ìƒì„¸ í˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘
            date, content = get_article_detail(article_url)
            
            collected_news.append({
                "title": title,
                "date": date,
                "content": content
            })
            print(f"   âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title[:25]}...")

        # JSON ì €ì¥ (íŒŒì¼ëª…ì€ ë¬´ì¡°ê±´ ì¢…ëª©ì½”ë“œ_ë‚ ì§œ_ì‹œê°„.json)
        file_name = f"{code}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        file_path = os.path.join(NEWS_DIR, file_name)
        
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(collected_news, f, ensure_ascii=False, indent=4)
        
        print(f"\nğŸ“‚ ì €ì¥ ì™„ë£Œ: {file_path}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    setup_directory()
    
    print("--- ì‹¤ì‹œê°„ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ---")
    target_code = input("ìˆ˜ì§‘í•  ì¢…ëª© ì½”ë“œ 6ìë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 005930): ").strip()
    
    # 6ìë¦¬ ìˆ«ìì¸ì§€ ê°„ë‹¨íˆ ì²´í¬
    if len(target_code) != 6 or not target_code.isdigit():
        print("âŒ ì˜¬ë°”ë¥¸ ì¢…ëª© ì½”ë“œê°€ ì•„ë‹™ë‹ˆë‹¤. 6ìë¦¬ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    fetch_stock_news_by_code(target_code)

if __name__ == "__main__":
    main()