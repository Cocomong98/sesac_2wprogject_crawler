import os
import json
import requests
import re
import time
from bs4 import BeautifulSoup
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

NEWS_DIR = "news"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
}

session = requests.Session()
session.headers.update(HEADERS)

client = OpenAI(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    base_url="https://api.upstage.ai/v1/solar"
)

def extract_article_id(url):
    """URLì—ì„œ oidì™€ aidë¥¼ ì¶”ì¶œí•˜ì—¬ ê³ ìœ  ID ìƒì„± (ì˜ˆ: 421_0008765615)"""
    oid_match = re.search(r"office_id=(\d+)", url) or re.search(r"article/(\d+)/", url)
    aid_match = re.search(r"article_id=(\d+)", url) or re.search(r"article/\d+/(\d+)", url)
    
    if oid_match and aid_match:
        return f"{oid_match.group(1)}_{aid_match.group(1)}"
    return None

def summarize_content(raw_content):
    if not raw_content or len(raw_content) < 50: return raw_content
    clean_text = " ".join(raw_content.split())[:2000]
    try:
        response = client.chat.completions.create(
            model="solar-pro",
            messages=[
                {"role": "system", "content": "ì£¼ì‹ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€. ì˜¤ì§ [ë…¼ì¡°], [ì¢…ëª©], [ë‚´ìš©]ë§Œ í¬í•¨ëœ ê²°ê³¼ë§Œ ì¤„ë°”ê¿ˆ ì—†ì´ ë‹µë³€."},
                {"role": "user", "content": f"ìš”ì•½í•´: {clean_text}"}
            ]
        )
        return response.choices[0].message.content.strip()
    except: return "ìš”ì•½ ì‹¤íŒ¨"

def get_article_data(url):
    """ê¸°ì‚¬ ë³¸ë¬¸ ë° ê³ ìœ  ì‹ë³„ê°’ ì¶”ì¶œ"""
    article_id = extract_article_id(url)
    if not article_id: return None

    # í‘œì¤€í™”ëœ ëª¨ë°”ì¼ ë‰´ìŠ¤ ì£¼ì†Œë¡œ ì ‘ì† (ì¶”ì¶œ ì„±ê³µë¥  ë†’ìŒ)
    oid, aid = article_id.split('_')
    standard_url = f"https://n.news.naver.com/mnews/article/{oid}/{aid}"

    try:
        res = session.get(standard_url, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        title_tag = soup.select_one('#title_area, .media_end_head_headline')
        date_tag = soup.select_one('._ARTICLE_DATE_TIME, .media_end_head_info_datestamp_time')
        content_tag = soup.select_one('#dic_area, #articleBodyContents')

        if title_tag and content_tag:
            for s in content_tag.select('script, style, .end_photo_org, .nbd_im_w'): s.decompose()
            return {
                "id": article_id,  # ì¤‘ë³µ ì²´í¬ìš© ê³ ìœ  ID ì €ì¥
                "title": title_tag.get_text(strip=True),
                "date": date_tag.get_text(strip=True) if date_tag else "ë‚ ì§œì—†ìŒ",
                "content": summarize_content(content_tag.get_text(" ", strip=True))
            }
    except: pass
    return None

# def run_crawler():
#     if not os.path.exists(NEWS_DIR): os.makedirs(NEWS_DIR)
    
#     print("\n--- ğŸ¤– ID ê¸°ë°˜ ì¦ë¶„ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬ ---")
#     code = input("ì¢…ëª© ì½”ë“œ 6ìë¦¬: ").strip()
#     if not (len(code) == 6 and code.isdigit()): return

#     today_str = datetime.now().strftime('%Y%m%d')
#     file_path = os.path.join(NEWS_DIR, f"{code}_{today_str}.json")
    
#     existing_data = []
#     existing_ids = set() # ì´ì œ ì œëª© ëŒ€ì‹  IDë¡œ ì¤‘ë³µ ì²´í¬

# í¬ë¡¤ëŸ¬ê°€ ì¼ë‹¨ ì‚¼ì„±ì „ìë§Œ í¬ë¡¤ë§í•˜ë„ë¡
def run_crawler():
    if not os.path.exists(NEWS_DIR): os.makedirs(NEWS_DIR)
    
    print("\n--- ğŸ¤– ID ê¸°ë°˜ ì¦ë¶„ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬ (ìë™ ëª¨ë“œ) ---")
    
    # ì‚¬ìš©ì ì…ë ¥ ëŒ€ì‹  ì‚¼ì„±ì „ì ì½”ë“œë¡œ ê³ ì •
    code = "005930" 
    print(f"ğŸ“ˆ ëŒ€ìƒ ì¢…ëª©: ì‚¼ì„±ì „ì({code})")

    today_str = datetime.now().strftime('%Y%m%d')
    file_path = os.path.join(NEWS_DIR, f"{code}_{today_str}.json")
    
    existing_data = []
    existing_ids = set() # ì´ì œ ì œëª© ëŒ€ì‹  IDë¡œ ì¤‘ë³µ ì²´í¬

    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            try:
                existing_data = json.load(f)
                # ê¸°ì¡´ ë°ì´í„°ì—ì„œ IDë§Œ ë½‘ì•„ì™€ì„œ Set êµ¬ì„±
                existing_ids = {item.get('id') for item in existing_data if item.get('id')}
                print(f"ğŸ“‚ ì˜¤ëŠ˜ ì´ë¯¸ {len(existing_ids)}ê±´ì˜ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except: pass

    # ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    list_url = f"https://finance.naver.com/item/news_news.naver?code={code}"
    session.headers.update({"Referer": f"https://finance.naver.com/item/main.naver?code={code}"})
    
    try:
        response = session.get(list_url, timeout=10)
        response.encoding = 'euc-kr'
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', class_='tit')
        
        new_items = []
        print(f"ğŸ” ìƒˆ ê¸°ì‚¬ í™•ì¸ ì¤‘...")

        for link_tag in reversed(links):
            article_href = link_tag.get('href', '')
            # ë§í¬ì—ì„œ ë¨¼ì € ID ì¶”ì¶œ
            current_id = extract_article_id(article_href)
            
            # IDê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìš”ì•½ ê³¼ì • ì—†ì´ ë°”ë¡œ íŒ¨ìŠ¤
            if current_id in existing_ids:
                continue

            if article_href.startswith('/'):
                article_href = "https://finance.naver.com" + article_href
            
            data = get_article_data(article_href)
            if data:
                print(f"   âœ¨ [NEW] {data['title'][:25]}...")
                new_items.append(data)
                existing_ids.add(data['id'])
                time.sleep(0.4)

        if new_items:
            final_data = existing_data + new_items
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(final_data, f, ensure_ascii=False, indent=4)
            print(f"\nğŸ‰ ì—…ë°ì´íŠ¸ ì™„ë£Œ! ({len(new_items)}ê±´ ì¶”ê°€ë¨)")
        else:
            print("\nâœ¨ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {e}")

if __name__ == "__main__":
    run_crawler()